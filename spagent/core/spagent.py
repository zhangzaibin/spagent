"""
SPAgent - Spatial Intelligence Agent

This module contains the main SPAgent class that orchestrates problem solving
using external expert tools and VLLM models.
"""

import json
import re
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Union
from pathlib import Path

from .tool import Tool, ToolRegistry
from .model import Model
from .prompts import create_system_prompt, create_follow_up_prompt, create_user_prompt, create_fallback_prompt
from .data_collector import DataCollector

logger = logging.getLogger(__name__)


class SPAgent:
    """
    Spatial Intelligence Agent
    
    An agent that can solve spatial intelligence problems by combining
    VLLM models with external expert tools.
    """
    
    def __init__(
        self, 
        model: Model,
        tools: Optional[List[Tool]] = None,
        max_workers: int = 4,
        data_collector: Optional[DataCollector] = None
    ):
        """
        Initialize SPAgent
        
        Args:
            model: VLLM model wrapper to use
            tools: List of external expert tools (optional)
            max_workers: Maximum number of parallel tool executions
            data_collector: Optional DataCollector for training data collection
        """
        self.model = model
        self.tool_registry = ToolRegistry()
        self.max_workers = max_workers
        self.data_collector = data_collector
        
        # Register provided tools
        if tools:
            for tool in tools:
                self.add_tool(tool)
        
        logger.info(f"Initialized SPAgent with model: {model.model_name}")
        if data_collector:
            logger.info("Data collection enabled")
    
    def add_tool(self, tool: Tool):
        """
        Add a tool to the agent
        
        Args:
            tool: Tool instance to add
        """
        self.tool_registry.register(tool)
    
    def remove_tool(self, tool_name: str):
        """
        Remove a tool from the agent
        
        Args:
            tool_name: Name of tool to remove
        """
        self.tool_registry.unregister(tool_name)
    
    def list_tools(self) -> List[str]:
        """
        Get list of available tool names
        
        Returns:
            List of tool names
        """
        return self.tool_registry.list_tools()
    
    def set_model(self, model: Model):
        """
        Set the VLLM model
        
        Args:
            model: Model instance to use
        """
        self.model = model
        logger.info(f"Updated model to: {model.model_name}")
    
    def solve_problem(
        self, 
        image_path: Union[str, List[str]], 
        question: str,
        max_iterations: int = 3,
        video_path: Optional[str] = None,
        pi3_target_fps: float = 1.0,
        use_baseline_comparison: bool = False,
        **model_kwargs
    ) -> Dict[str, Any]:
        """
        Solve a spatial intelligence problem
        
        Args:
            image_path: Path to image or list of image paths
            question: User's question about the image(s)
            max_iterations: Maximum number of tool-call iterations (default: 1)
            video_path: Optional path to original video (for pi3 tool re-sampling)
            pi3_target_fps: Target FPS for pi3 tool frame extraction (default 1.0)
            use_baseline_comparison: If True, run a naive baseline (no tools) in parallel
                                    and synthesize final answer from both results (default: False)
            **model_kwargs: Additional arguments for model inference
            
        Returns:
            Dictionary containing:
            - answer: Final answer text
            - initial_response: Model's initial response
            - tool_calls: List of tool calls made
            - tool_results: Results from tool execution
            - used_tools: List of tools that were used
            - additional_images: List of additional images generated by tools
            - iterations: Number of iterations performed
            - baseline_answer: Naive baseline answer (if use_baseline_comparison=True)
        """
        logger.info(f"Starting problem solving for question: {question} (max_iterations={max_iterations})")
        
        # Validate inputs
        if isinstance(image_path, str):
            image_paths = [image_path]
        else:
            image_paths = image_path
        
        for path in image_paths:
            if not Path(path).exists():
                raise FileNotFoundError(f"Image not found: {path}")
        
        # Start data collection session if enabled
        if self.data_collector:
            self.data_collector.start_session(question, image_paths)
        
        # Create system prompt with available tools
        tool_schemas = self.tool_registry.get_function_schemas()
        system_prompt = create_system_prompt(tool_schemas)
        # system_prompt += '\nThis time, you need to call the function anyway in <tool_call></tool_call> format.' # Debug Only!
        user_prompt = create_user_prompt(question, image_paths, tool_schemas)
        
        # Initialize tracking variables for multi-step workflow
        all_tool_calls = []
        all_tool_results = {}
        all_additional_images = []
        all_successful_tools = []
        current_images = image_paths
        initial_response = None
        last_response = None
        iteration = 0
        baseline_answer = None  # For naive baseline comparison
        baseline_triggered = False  # Track if baseline has been triggered
        
        # Multi-step workflow loop
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"=== Iteration {iteration}/{max_iterations} ===")
            
            # Step 1: Get model response
            if iteration == 1:
                # First iteration: use initial prompt
                prompt = system_prompt + "\n\n" + user_prompt
                logger.info("Getting initial model response...")
            else:
                # Subsequent iterations: create continuation prompt
                prompt = self._create_continuation_prompt(
                    question, 
                    last_response, 
                    all_tool_results, 
                    image_paths,
                    all_additional_images,
                    iteration,
                    max_iterations
                )
                logger.info(f"Getting continuation response for iteration {iteration}...")
            
            if len(current_images) == 1:
                current_response = self.model.single_image_inference(
                    current_images[0], 
                    prompt,
                    **model_kwargs
                )
            else:
                current_response = self.model.multiple_images_inference(
                    current_images,
                    prompt,
                    **model_kwargs
                )
            
            logger.info(f"Response: {current_response}")
            
            # Save initial response
            if iteration == 1:
                initial_response = current_response
            last_response = current_response
            
            # Record inference data if collection is enabled
            if self.data_collector:
                self.data_collector.record_inference(
                    iteration=iteration,
                    images=current_images,
                    prompt=prompt,
                    response=current_response,
                    context={
                        "tool_calls_history": all_tool_calls,
                        "tool_results_history": all_tool_results,
                        "additional_images_history": all_additional_images
                    }
                )
            
            # Step 2: Parse tool calls from response
            tool_calls = self._parse_tool_calls(current_response)
            
            # Trigger naive baseline if enabled and tool calls detected
            if use_baseline_comparison and tool_calls and not baseline_triggered:
                baseline_triggered = True
                logger.info("Tool calls detected - triggering naive baseline agent...")
                baseline_answer = self._get_naive_baseline_answer(image_paths, question, **model_kwargs)
                logger.info(f"Naive baseline answer: {baseline_answer}")
            
            # Check if response has <answer> tags (indicating completion)
            has_answer = self._has_answer_tags(current_response)
            
            if not tool_calls:
                logger.info(f"No tool calls found in iteration {iteration}")
                if has_answer or iteration == max_iterations:
                    logger.info("Ending workflow: final answer provided or max iterations reached")
                    break
                # If no tool calls and no answer, continue to next iteration
                continue
            
            # Step 3: Execute tools
            logger.info(f"Executing {len(tool_calls)} tool calls in iteration {iteration}...")
            tool_results = self._execute_tools(tool_calls, video_path=video_path, pi3_target_fps=pi3_target_fps)
            
            # Step 4: Collect results
            iteration_additional_images = []
            for tool_name, result in tool_results.items():
                if result.get('success'):
                    all_successful_tools.append(f"{tool_name}_iter{iteration}")
                    # Collect additional images
                    if 'output_path' in result and result['output_path'] is not None:
                        if Path(result['output_path']).exists():
                            iteration_additional_images.append(result['output_path'])
                    if 'vis_path' in result and result['vis_path'] is not None:
                        if Path(result['vis_path']).exists():
                            iteration_additional_images.append(result['vis_path'])
            
            # Update tracking variables
            all_tool_calls.extend(tool_calls)
            all_tool_results.update({f"{k}_iter{iteration}": v for k, v in tool_results.items()})
            all_additional_images.extend(iteration_additional_images)
            
            # Update current images for next iteration
            if iteration_additional_images:
                # Combine original images with new images generated by tools
                valid_images = self._sort_additional_images_by_input_order(image_paths, iteration_additional_images)
                if valid_images:
                    # Keep original images + add new images for comprehensive analysis
                    current_images = image_paths + valid_images
                # else: keep current_images unchanged
            
            # Check if we should stop (has answer tag and we're not forcing more iterations)
            if has_answer and iteration < max_iterations:
                logger.info(f"Answer provided in iteration {iteration}, but continuing workflow if more tool calls exist")
        
        # Generate final response if needed
        if not self._has_answer_tags(last_response) and all_successful_tools:
            logger.info("Generating final response...")
            tool_description = None
            if all_tool_results:
                last_result = list(all_tool_results.values())[-1]
                if last_result.get('description'):
                    tool_description = last_result.get('description')
            
            follow_up_prompt = create_follow_up_prompt(
                question, 
                initial_response, 
                all_tool_results,
                image_paths,
                all_additional_images,
                tool_description
            )
            
            valid_additional_images = self._sort_additional_images_by_input_order(image_paths, all_additional_images)
            final_images = valid_additional_images if valid_additional_images else image_paths
            
            if len(final_images) == 1:
                final_response = self.model.single_image_inference(
                    final_images[0],
                    follow_up_prompt,
                    **model_kwargs
                )
            else:
                final_response = self.model.multiple_images_inference(
                    final_images,
                    follow_up_prompt,
                    **model_kwargs
                )
            
            # Record final synthesis inference if collection is enabled
            if self.data_collector:
                self.data_collector.record_inference(
                    iteration=iteration + 1,  # Final synthesis is an additional step
                    images=final_images,
                    prompt=follow_up_prompt,
                    response=final_response,
                    context={
                        "type": "final_synthesis",
                        "tool_calls_history": all_tool_calls,
                        "tool_results_history": all_tool_results,
                        "additional_images_history": all_additional_images
                    }
                )
                
        elif not self._has_answer_tags(last_response):
            # Generate fallback if no answer tags
            logger.warning("No answer tags found, generating fallback response")
            fallback_prompt = create_fallback_prompt(question, last_response)
            if len(current_images) == 1:
                final_response = self.model.single_image_inference(
                    current_images[0],
                    fallback_prompt,
                    **model_kwargs
                )
            else:
                final_response = self.model.multiple_images_inference(
                    current_images,
                    fallback_prompt,
                    **model_kwargs
                )
            
            # Record fallback inference if collection is enabled
            if self.data_collector:
                self.data_collector.record_inference(
                    iteration=iteration + 1,
                    images=current_images,
                    prompt=fallback_prompt,
                    response=final_response,
                    context={
                        "type": "fallback",
                        "tool_calls_history": all_tool_calls,
                        "tool_results_history": all_tool_results
                    }
                )
        else:
            final_response = last_response
        
        # Synthesize final answer with baseline comparison if enabled
        if use_baseline_comparison and baseline_answer is not None:
            logger.info("Synthesizing final answer from tool-based and baseline responses...")
            baseline_response = final_response  # Save pre-synthesis response
            final_response = self._synthesize_with_baseline(
                question, 
                final_response, 
                baseline_answer, 
                image_paths,
                all_additional_images,
                **model_kwargs
            )
            
            # Record baseline synthesis inference if collection is enabled
            if self.data_collector:
                valid_additional_images = self._sort_additional_images_by_input_order(image_paths, all_additional_images)
                final_images = valid_additional_images if valid_additional_images else image_paths
                self.data_collector.record_inference(
                    iteration=iteration + 2,  # Baseline synthesis is an additional step
                    images=final_images,
                    prompt=f"Synthesizing tool-based and baseline answers for: {question}",
                    response=final_response,
                    context={
                        "type": "baseline_synthesis",
                        "tool_based_answer": baseline_response,
                        "baseline_answer": baseline_answer,
                        "tool_calls_history": all_tool_calls,
                        "tool_results_history": all_tool_results
                    }
                )
        
        # Clean up temporary pi3 frames
        self._cleanup_pi3_frames()
        
        # End data collection session if enabled
        if self.data_collector:
            # Extract final answer from response
            extracted_answer = self._extract_answer(final_response)
            success = extracted_answer is not None  # Success if we have an answer
            
            self.data_collector.end_session(
                success=success,
                final_answer=extracted_answer or final_response,
                error_message=None if success else "No answer tags found",
                metadata={
                    "iterations": iteration,
                    "num_tool_calls": len(all_tool_calls),
                    "used_tools": all_successful_tools,
                    "num_additional_images": len(all_additional_images)
                }
            )
        
        return {
            "answer": final_response,
            "initial_response": initial_response or final_response,
            "tool_calls": all_tool_calls,
            "tool_results": all_tool_results,
            "used_tools": all_successful_tools,
            "additional_images": all_additional_images,
            "iterations": iteration,
            "baseline_answer": baseline_answer,  # Naive baseline answer (if enabled)
            "prompts": {
                "system_prompt": system_prompt,
                "user_prompt": user_prompt,
                "follow_up_prompt": None  # This is now handled in continuation prompts
            }
        }
    
    def _parse_tool_calls(self, response: str) -> List[Dict[str, Any]]:
        """
        Parse tool calls from model response
        
        Args:
            response: Model response text
            
        Returns:
            List of tool call dictionaries
        """
        tool_calls = []
        
        # Find all tool_call blocks
        pattern = r'<tool_call>\s*({.*?})\s*</tool_call>'
        matches = re.findall(pattern, response, re.DOTALL)
        
        for match in matches:
            try:
                tool_call = json.loads(match)
                if 'name' in tool_call and 'arguments' in tool_call:
                    tool_calls.append(tool_call)
                else:
                    logger.warning(f"Invalid tool call format: {match}")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse tool call JSON: {match}, error: {e}")
        
        return tool_calls
    
    def _execute_tools(self, tool_calls: List[Dict[str, Any]], video_path: Optional[str] = None, pi3_target_fps: float = 1.0) -> Dict[str, Any]:
        """
        Execute tool calls in parallel when possible
        
        Args:
            tool_calls: List of tool call dictionaries
            video_path: Optional path to original video (for pi3 tool re-sampling)
            pi3_target_fps: Target FPS for pi3 tool frame extraction
            
        Returns:
            Dictionary of tool_name -> result
        """
        tool_results = {}
        
        # Group tool calls by tool name to handle multiple calls to same tool
        tool_groups = {}
        for i, call in enumerate(tool_calls):
            tool_name = call['name']
            if tool_name not in tool_groups:
                tool_groups[tool_name] = []
            tool_groups[tool_name].append((i, call))
        
        # Execute tools in parallel, but Pi3Tool and Pi3MultiimgTool sequentially to avoid server issues
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_tool = {}
            
            # Handle Pi3Tool and Pi3MultiimgTool calls sequentially first
            pi3_calls = []
            other_calls = {}
            
            for tool_name, calls in tool_groups.items():
                if tool_name in ['pi3_tool', 'pi3_multiimg_tool']:
                    pi3_calls.extend(calls)
                else:
                    other_calls[tool_name] = calls
            
            # Execute Pi3Tool and Pi3MultiimgTool calls sequentially
            if pi3_calls:
                logger.info(f"Executing {len(pi3_calls)} Pi3 tool calls sequentially...")
                
                # Extract more frames for pi3 if video_path is provided
                pi3_frame_paths = []
                if video_path and Path(video_path).exists():
                    logger.info(f"Extracting frames for pi3 tool from video: {video_path} at {pi3_target_fps} fps")
                    pi3_frame_paths = self._extract_frames_for_pi3(video_path, pi3_target_fps)
                
                for call_idx, call in pi3_calls:
                    tool_name = call['name']
                    tool = self.tool_registry.get(tool_name)
                    if tool:
                        # If we extracted frames for pi3, update the arguments
                        arguments = call['arguments'].copy()
                        if pi3_frame_paths:
                            # Update image_path in arguments
                            if 'image_path' in arguments:
                                logger.info(f"Updating pi3 tool arguments with {len(pi3_frame_paths)} newly extracted frames")
                                arguments['image_path'] = pi3_frame_paths
                        
                        result = self._safe_tool_call(tool, arguments)
                        result_key = tool_name if len(pi3_calls) == 1 else f"{tool_name}_{call_idx}"
                        tool_results[result_key] = result
                        # Add small delay between Pi3 tool calls
                        import time
                        time.sleep(1)
                    else:
                        logger.error(f"{tool_name} not found")
                        result_key = tool_name if len(pi3_calls) == 1 else f"{tool_name}_{call_idx}"
                        tool_results[result_key] = {
                            "success": False,
                            "error": f"{tool_name} not found"
                        }
            
            # Execute other tools in parallel as before
            for tool_name, calls in other_calls.items():
                tool = self.tool_registry.get(tool_name)
                if tool is None:
                    logger.error(f"Tool not found: {tool_name}")
                    for _, call in calls:
                        tool_results[f"{tool_name}_{_}"] = {
                            "success": False,
                            "error": f"Tool not found: {tool_name}"
                        }
                    continue
                
                # Submit tool execution for each call
                for call_idx, call in calls:
                    future = executor.submit(self._safe_tool_call, tool, call['arguments'])
                    future_to_tool[future] = (tool_name, call_idx)
            
            # Collect results from parallel execution
            for future in as_completed(future_to_tool):
                tool_name, call_idx = future_to_tool[future]
                try:
                    result = future.result()
                    # Use unique key for multiple calls to same tool
                    result_key = tool_name if len([t for t in other_calls.get(tool_name, [])]) == 1 else f"{tool_name}_{call_idx}"
                    tool_results[result_key] = result
                except Exception as e:
                    logger.error(f"Tool execution failed for {tool_name}: {e}")
                    result_key = tool_name if len([t for t in other_calls.get(tool_name, [])]) == 1 else f"{tool_name}_{call_idx}"
                    tool_results[result_key] = {
                        "success": False,
                        "error": str(e)
                    }
        
        return tool_results
    
    def _safe_tool_call(self, tool: Tool, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Safely execute a tool call with error handling
        
        Args:
            tool: Tool instance
            arguments: Tool arguments
            
        Returns:
            Tool execution result
        """
        try:
            logger.info(f"Executing tool: {tool.name} with args: {arguments}")
            result = tool.call(**arguments)
            logger.info(f"Tool {tool.name} completed successfully")
            return result
        except Exception as e:
            logger.error(f"Tool {tool.name} execution failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _sort_additional_images_by_input_order(self, image_paths: List[str], additional_images: List[str]) -> List[str]:
        """
        Sort additional images to match the order of input image_paths
        
        Args:
            image_paths: Original input image paths in order
            additional_images: Generated additional images (may be in different order)
            
        Returns:
            List of valid additional images sorted by input order
        """
        # Filter out None and invalid paths first
        valid_additional_images = [img for img in additional_images if img is not None and Path(img).exists()]
        
        if not valid_additional_images:
            return []
        
        # Create a sorted list of additional images based on input order
        sorted_additional_images = []
        
        for input_path in image_paths:
            input_stem = Path(input_path).stem
            
            # Find all additional images that correspond to this input image
            matching_images = []
            
            for additional_img in valid_additional_images:
                if additional_img in sorted_additional_images:
                    continue  # Already matched
                
                additional_stem = Path(additional_img).stem
                
                # Check if this additional image corresponds to the current input image
                if self._is_image_match(input_stem, additional_stem):
                    matching_images.append(additional_img)
            
            # Add matching images in a consistent order (by filename)
            matching_images.sort()
            sorted_additional_images.extend(matching_images)
        
        # Add any remaining unmatched additional images at the end
        for additional_img in valid_additional_images:
            if additional_img not in sorted_additional_images:
                sorted_additional_images.append(additional_img)
        
        logger.info(f"Sorted additional images: {[Path(img).name for img in sorted_additional_images]}")
        return sorted_additional_images
    
    def _is_image_match(self, input_stem: str, additional_stem: str) -> bool:
        """
        Check if an additional image corresponds to an input image
        
        Args:
            input_stem: Stem of input image filename
            additional_stem: Stem of additional image filename
            
        Returns:
            True if they match, False otherwise
        """
        # Since the suffix of saved images is always the same as the original image name,
        # we check if the additional image name ends with the input image name
        # This handles any prefix automatically without needing to maintain a list
        return additional_stem.endswith(input_stem)
    
    def _has_answer_tags(self, response: str) -> bool:
        """
        Check if response contains <answer> tags
        
        Args:
            response: Response text to check
            
        Returns:
            True if response contains <answer> tags, False otherwise
        """
        return '<answer>' in response and '</answer>' in response
    
    def _extract_answer(self, response: str) -> Optional[str]:
        """
        Extract answer content from <answer> tags
        
        Args:
            response: Response text to extract from
            
        Returns:
            Extracted answer text or None if no answer tags found
        """
        pattern = r'<answer>(.*?)</answer>'
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return match.group(1).strip()
        return None
    
    def _create_continuation_prompt(
        self, 
        question: str, 
        last_response: str, 
        all_tool_results: Dict[str, Any],
        original_images: List[str],
        additional_images: List[str],
        current_iteration: int,
        max_iterations: int
    ) -> str:
        """
        Create continuation prompt for multi-step workflow
        
        Args:
            question: Original user question
            last_response: Last model response
            all_tool_results: All tool results so far
            original_images: Original image paths
            additional_images: All additional images generated
            current_iteration: Current iteration number
            max_iterations: Maximum iterations allowed
            
        Returns:
            Continuation prompt string
        """
        tool_summary = []
        angle_info = []
        
        for tool_name, result in all_tool_results.items():
            if result.get('success'):
                tool_summary.append(f"- {tool_name}: Successfully executed")
                
                # Extract angle information if available
                if 'azimuth_angle' in result and 'elevation_angle' in result:
                    azim = result.get('azimuth_angle')
                    elev = result.get('elevation_angle')
                    angle_info.append(f"  └─ Viewing angle: azimuth={azim}°, elevation={elev}°")
                
                if result.get('description'):
                    tool_summary.append(f"  Description: {result.get('description')}")
            else:
                tool_summary.append(f"- {tool_name}: Failed - {result.get('error', 'Unknown error')}")
        
        tool_summary_text = "\n".join(tool_summary) if tool_summary else "None yet"
        angle_info_text = "\n".join(angle_info) if angle_info else ""
        
        original_images_info = "\n".join([f"- {path}" for path in original_images]) if original_images else "None"
        additional_images_info = "\n".join([f"- {path}" for path in additional_images]) if additional_images else "None yet"
        
        remaining = max_iterations - current_iteration
        
        prompt = f"""=== Multi-Step Analysis: Iteration {current_iteration}/{max_iterations} ===

Original Question: {question}

Your Previous Response: 
{last_response}

Tool Execution Summary:
{tool_summary_text}
{angle_info_text}

Original Images:
{original_images_info}

Generated Images Available for Analysis:
{additional_images_info}

=== Next Steps ===

You have {remaining} more iteration(s) available. You can:

1. **Continue investigating** - Call tools with DIFFERENT parameters:
   - For Pi3 tools: Try different viewing angles to understand the 3D structure better
   - Example angles: front (0,0), left (-45,0), right (45,0), top (0,45), bottom (0,-45), back (180,0)
   - Each angle reveals different aspects of the 3D structure

2. **Provide final answer** - If you have sufficient information from current viewpoints:
   - Output your comprehensive analysis in <think></think> tags.
   - Reference the specific viewpoints that helped you understand the structure

Instructions:
- Think: Do you need to see the object from another angle to answer the question better?
- If YES: Use <tool_call></tool_call> to request a different viewing angle
- If NO: output your thinking process in <think></think> and your final answer in <answer></answer>. Only put Options in <answer></answer> tags, do not put any other text.

Please continue:"""
        
        return prompt
    def _extract_frames_for_pi3(self, video_path: str, target_fps: float = 1.0) -> List[str]:
        """
        Extract frames from video for pi3 tool
        
        Args:
            video_path: Path to video file
            target_fps: Target frame rate
            
        Returns:
            List of paths to extracted frame images
        """
        try:
            import cv2
            import numpy as np
        except ImportError:
            logger.error("cv2 is required for video frame extraction. Please install opencv-python.")
            return []
        
        cap = cv2.VideoCapture(video_path)
        
        # Get video info
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        original_fps = cap.get(cv2.CAP_PROP_FPS)
        total_duration = total_frames / original_fps
        
        # Calculate frames to extract based on target fps
        num_frames = int(total_duration * target_fps)
        frame_interval = total_frames / num_frames
        
        frame_paths = []
        temp_dir = Path("temp_frames_pi3")
        temp_dir.mkdir(exist_ok=True)
        
        # Extract video filename (without extension)
        video_filename = Path(video_path).stem
        
        # Extract frames evenly
        for i in range(num_frames):
            frame_idx = int(i * frame_interval)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frame_path = temp_dir / f"{video_filename}_pi3_frame_{i}.jpg"
                cv2.imwrite(str(frame_path), frame)
                frame_paths.append(str(frame_path))
        
        cap.release()
        logger.info(f"Extracted {len(frame_paths)} frames from video for pi3 tool (duration: {total_duration:.2f}s, original fps: {original_fps:.2f}, target fps: {target_fps})")
        return frame_paths
    
    def _cleanup_pi3_frames(self):
        """
        Clean up temporary frames extracted for pi3 tool
        """
        import os
        import shutil
        
        temp_dir = Path("temp_frames_pi3")
        if temp_dir.exists():
            try:
                shutil.rmtree(temp_dir)
                logger.info("Cleaned up temporary pi3 frames")
            except Exception as e:
                logger.warning(f"Failed to clean up temporary pi3 frames: {e}")
    
    def _get_naive_baseline_answer(
        self, 
        image_paths: List[str], 
        question: str,
        **model_kwargs
    ) -> str:
        """
        Get answer from naive baseline agent (no tools)
        
        Args:
            image_paths: List of image paths
            question: User's question
            **model_kwargs: Additional arguments for model inference
            
        Returns:
            Baseline answer string
        """
        # Create simple prompt without tool information
        naive_prompt = f"""You are a helpful AI assistant specialized in spatial intelligence and visual reasoning.

Please analyze the image(s) and answer the following question directly, using only what you can see in the image(s).

Question: {question}

Please provide your answer directly in <answer></answer> tags."""
        
        try:
            if len(image_paths) == 1:
                response = self.model.single_image_inference(
                    image_paths[0],
                    naive_prompt,
                    **model_kwargs
                )
            else:
                response = self.model.multiple_images_inference(
                    image_paths,
                    naive_prompt,
                    **model_kwargs
                )
            return response
        except Exception as e:
            logger.error(f"Failed to get naive baseline answer: {e}")
            return f"Error: {str(e)}"
    
    def _synthesize_with_baseline(
        self, 
        question: str,
        tool_based_answer: str,
        baseline_answer: str,
        image_paths: List[str],
        additional_images: List[str],
        **model_kwargs
    ) -> str:
        """
        Synthesize final answer by comparing tool-based and baseline answers
        
        Args:
            question: Original question
            tool_based_answer: Answer from tool-enhanced reasoning
            baseline_answer: Answer from naive baseline (no tools)
            image_paths: Original image paths
            additional_images: Additional images generated by tools
            **model_kwargs: Additional arguments for model inference
            
        Returns:
            Synthesized final answer
        """
        synthesis_prompt = f"""You are a helpful AI assistant tasked with providing the most accurate answer by synthesizing information from two different approaches.

Original Question: {question}

Approach 1 - Tool-Enhanced Analysis:
{tool_based_answer}

Approach 2 - Direct Visual Analysis (Baseline):
{baseline_answer}

=== Your Task ===

Compare the two approaches and provide the MOST ACCURATE answer to the original question.

Instructions:
1. Carefully analyze both answers
2. Consider which approach provides more reliable information:
   - Tool-enhanced analysis may have additional insights from specialized tools (depth, 3D, segmentation, etc.)
   - Direct visual analysis relies purely on what's visible in the image
3. Identify where they agree or disagree
4. When they disagree, determine which is more likely to be correct and why
5. Synthesize the information to provide the best possible answer

Important:
- If both approaches agree, confirm the answer with confidence
- If they disagree, explain your reasoning for choosing one over the other
- Use information from the tool-enhanced approach when it provides clear additional insights
- Use information from the baseline when the tool-enhanced approach seems uncertain or incorrect
- Your goal is to provide the MOST ACCURATE answer, not just combine both answers

Please provide your final synthesized answer in <answer></answer> tags."""
        
        try:
            # Use additional images if available, otherwise use original images
            valid_additional_images = self._sort_additional_images_by_input_order(image_paths, additional_images)
            final_images = valid_additional_images if valid_additional_images else image_paths
            
            if len(final_images) == 1:
                response = self.model.single_image_inference(
                    final_images[0],
                    synthesis_prompt,
                    **model_kwargs
                )
            else:
                response = self.model.multiple_images_inference(
                    final_images,
                    synthesis_prompt,
                    **model_kwargs
                )
            
            logger.info(f"Synthesized answer: {response}")
            return response
        except Exception as e:
            logger.error(f"Failed to synthesize answer: {e}")
            # Fallback to tool-based answer if synthesis fails
            return tool_based_answer
