"""
SPAgent - Spatial Intelligence Agent

This module contains the main SPAgent class that orchestrates problem solving
using external expert tools and VLLM models.
"""

import json
import re
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Union
from pathlib import Path

from .tool import Tool, ToolRegistry
from .model import Model
from .prompts import create_system_prompt, create_follow_up_prompt, create_user_prompt

logger = logging.getLogger(__name__)


class SPAgent:
    """
    Spatial Intelligence Agent
    
    An agent that can solve spatial intelligence problems by combining
    VLLM models with external expert tools.
    """
    
    def __init__(
        self, 
        model: Model,
        tools: Optional[List[Tool]] = None,
        max_workers: int = 4
    ):
        """
        Initialize SPAgent
        
        Args:
            model: VLLM model wrapper to use
            tools: List of external expert tools (optional)
            max_workers: Maximum number of parallel tool executions
        """
        self.model = model
        self.tool_registry = ToolRegistry()
        self.max_workers = max_workers
        
        # Register provided tools
        if tools:
            for tool in tools:
                self.add_tool(tool)
        
        logger.info(f"Initialized SPAgent with model: {model.model_name}")
    
    def add_tool(self, tool: Tool):
        """
        Add a tool to the agent
        
        Args:
            tool: Tool instance to add
        """
        self.tool_registry.register(tool)
    
    def remove_tool(self, tool_name: str):
        """
        Remove a tool from the agent
        
        Args:
            tool_name: Name of tool to remove
        """
        self.tool_registry.unregister(tool_name)
    
    def list_tools(self) -> List[str]:
        """
        Get list of available tool names
        
        Returns:
            List of tool names
        """
        return self.tool_registry.list_tools()
    
    def set_model(self, model: Model):
        """
        Set the VLLM model
        
        Args:
            model: Model instance to use
        """
        self.model = model
        logger.info(f"Updated model to: {model.model_name}")
    
    def solve_problem(
        self, 
        image_path: Union[str, List[str]], 
        question: str,
        **model_kwargs
    ) -> Dict[str, Any]:
        """
        Solve a spatial intelligence problem
        
        Args:
            image_path: Path to image or list of image paths
            question: User's question about the image(s)
            **model_kwargs: Additional arguments for model inference
            
        Returns:
            Dictionary containing:
            - answer: Final answer text
            - initial_response: Model's initial response
            - tool_calls: List of tool calls made
            - tool_results: Results from tool execution
            - used_tools: List of tools that were used
            - additional_images: List of additional images generated by tools
        """
        logger.info(f"Starting problem solving for question: {question}")
        
        # Validate inputs
        if isinstance(image_path, str):
            image_paths = [image_path]
        else:
            image_paths = image_path
        
        for path in image_paths:
            if not Path(path).exists():
                raise FileNotFoundError(f"Image not found: {path}")
        
        # Create system prompt with available tools
        tool_schemas = self.tool_registry.get_function_schemas()
        system_prompt = create_system_prompt(tool_schemas)
        user_prompt = create_user_prompt(question)
        
        # Step 1: Get initial model response
        logger.info("Getting initial model response...")
        if len(image_paths) == 1:
            initial_response = self.model.single_image_inference(
                image_paths[0], 
                system_prompt + "\n\n" + user_prompt,
                **model_kwargs
            )
        else:
            initial_response = self.model.multiple_images_inference(
                image_paths,
                system_prompt + "\n\n" + user_prompt,
                **model_kwargs
            )
        
        logger.info(f"Initial response: {initial_response}")
        
        # Step 2: Parse tool calls from response
        tool_calls = self._parse_tool_calls(initial_response)
        
        if not tool_calls:
            logger.info("No tool calls found, returning initial response")
            return {
                "answer": initial_response,
                "initial_response": initial_response,
                "tool_calls": [],
                "tool_results": {},
                "used_tools": [],
                "additional_images": []
            }
        
        # Step 3: Execute tools
        logger.info(f"Executing {len(tool_calls)} tool calls...")
        tool_results = self._execute_tools(tool_calls)
        
        # Step 4: Collect successful tool results and additional images
        successful_tools = []
        additional_images = []
        
        for tool_name, result in tool_results.items():
            if result.get('success'):
                successful_tools.append(tool_name)
                # Collect additional images from tool results (filter out None values)
                if 'output_path' in result and result['output_path'] is not None:
                    if Path(result['output_path']).exists():
                        additional_images.append(result['output_path'])
                if 'vis_path' in result and result['vis_path'] is not None:
                    if Path(result['vis_path']).exists():
                        additional_images.append(result['vis_path'])
        
        # Step 5: Generate final response with tool results
        if successful_tools:
            logger.info(f"Generating final response with results from {len(successful_tools)} tools...")
            follow_up_prompt = create_follow_up_prompt(question, initial_response, tool_results)
            
            # Include additional images in final inference if available (filter out None and invalid paths)
            valid_additional_images = [img for img in additional_images if img is not None and Path(img).exists()]
            all_images = image_paths + valid_additional_images
            
            if len(all_images) == 1:
                final_response = self.model.single_image_inference(
                    all_images[0],
                    follow_up_prompt,
                    **model_kwargs
                )
            else:
                final_response = self.model.multiple_images_inference(
                    all_images,
                    follow_up_prompt,
                    **model_kwargs
                )
        else:
            logger.warning("No tools executed successfully, using initial response")
            final_response = initial_response
        
        return {
            "answer": final_response,
            "initial_response": initial_response,
            "tool_calls": tool_calls,
            "tool_results": tool_results,
            "used_tools": successful_tools,
            "additional_images": additional_images
        }
    
    def _parse_tool_calls(self, response: str) -> List[Dict[str, Any]]:
        """
        Parse tool calls from model response
        
        Args:
            response: Model response text
            
        Returns:
            List of tool call dictionaries
        """
        tool_calls = []
        
        # Find all tool_call blocks
        pattern = r'<tool_call>\s*({.*?})\s*</tool_call>'
        matches = re.findall(pattern, response, re.DOTALL)
        
        for match in matches:
            try:
                tool_call = json.loads(match)
                if 'name' in tool_call and 'arguments' in tool_call:
                    tool_calls.append(tool_call)
                else:
                    logger.warning(f"Invalid tool call format: {match}")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse tool call JSON: {match}, error: {e}")
        
        return tool_calls
    
    def _execute_tools(self, tool_calls: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Execute tool calls in parallel when possible
        
        Args:
            tool_calls: List of tool call dictionaries
            
        Returns:
            Dictionary of tool_name -> result
        """
        tool_results = {}
        
        # Group tool calls by tool name to handle multiple calls to same tool
        tool_groups = {}
        for i, call in enumerate(tool_calls):
            tool_name = call['name']
            if tool_name not in tool_groups:
                tool_groups[tool_name] = []
            tool_groups[tool_name].append((i, call))
        
        # Execute tools in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_tool = {}
            
            for tool_name, calls in tool_groups.items():
                tool = self.tool_registry.get(tool_name)
                if tool is None:
                    logger.error(f"Tool not found: {tool_name}")
                    for _, call in calls:
                        tool_results[f"{tool_name}_{_}"] = {
                            "success": False,
                            "error": f"Tool not found: {tool_name}"
                        }
                    continue
                
                # Submit tool execution for each call
                for call_idx, call in calls:
                    future = executor.submit(self._safe_tool_call, tool, call['arguments'])
                    future_to_tool[future] = (tool_name, call_idx)
            
            # Collect results
            for future in as_completed(future_to_tool):
                tool_name, call_idx = future_to_tool[future]
                try:
                    result = future.result()
                    # Use unique key for multiple calls to same tool
                    result_key = tool_name if len([t for t in tool_groups[tool_name]]) == 1 else f"{tool_name}_{call_idx}"
                    tool_results[result_key] = result
                except Exception as e:
                    logger.error(f"Tool execution failed for {tool_name}: {e}")
                    result_key = tool_name if len([t for t in tool_groups[tool_name]]) == 1 else f"{tool_name}_{call_idx}"
                    tool_results[result_key] = {
                        "success": False,
                        "error": str(e)
                    }
        
        return tool_results
    
    def _safe_tool_call(self, tool: Tool, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Safely execute a tool call with error handling
        
        Args:
            tool: Tool instance
            arguments: Tool arguments
            
        Returns:
            Tool execution result
        """
        try:
            logger.info(f"Executing tool: {tool.name} with args: {arguments}")
            result = tool.call(**arguments)
            logger.info(f"Tool {tool.name} completed successfully")
            return result
        except Exception as e:
            logger.error(f"Tool {tool.name} execution failed: {e}")
            return {
                "success": False,
                "error": str(e)
            } 