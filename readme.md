# ğŸ“Œ Introduction

This repository integrates **agentic skills** into **multi-modal understanding** using external expert models and LLMs.

---

## ğŸ“‚ Project Structure

| Module | Path | Description |
|--------|------|-------------|
| **External Experts** | `spagent/external_experts/` | Specialized models for spatial intelligence:<br>- Depth Estimation (**Depth-AnythingV2**)<br>- Object Detection & Segmentation (**SAM2**)<br>- Can run as external APIs |
| **VLLM Models** | `spagent/vllm_models/` | VLLM inference functions & wrappers:<br>- GPT / QwenVL inference<br>- Model loading & serving utilities<br>- Unified API for LLM calls |
| **Workflows** | `spagent/workflows/` | Orchestrates complete workflows:<br>- Combines LLM + external experts<br>- Defines spatial reasoning pipelines<br>- Manages data flow |
| **Examples** | `spagent/examples/` | Example scripts, each showing a usage tutorial (e.g., `depth_workflow_example_usage.py`) |

---

## ğŸš€ Quick Start

### 1 Prepare APIs
```bash
# OpenAI API
export OPENAI_API_KEY="your_api_key"
export OPENAI_BASE_URL="http://35.220.164.252:3888/v1/"

# Qwen API (apply at https://bailian.console.aliyun.com)
export DASHSCOPE_API_KEY="your_api_key"

# Test Qwen API
python spagent/vllm_models/qwen.py

# prepare VLLM in the iiau A800 server
vllm serve /13693266743/models/Qwen2.5-VL-7B-Instruct --host 0.0.0.0 --port 20004 --served-model-name 'qwen-vl' 

# Then, you can run
python spagent/vllm_models/qwen_vllm.py

# ç°åœ¨æˆ‘å·²ç»åœ¨A800ä¸Šéƒ¨ç½²äº†ï¼Œipä»€ä¹ˆéƒ½æ˜¯å›ºå®šçš„ï¼Œç›´æ¥è·‘å°±è¡Œï¼Œ24å°æ—¶å†…éƒ½èƒ½ç”¨ï¼Œè¿‡æ—¶é—´æˆ‘å†éƒ¨ç½²ã€‚
```





### 2 Install
```
# å®‰è£…çš„åŒ…å¾ˆå°‘ï¼Œä¸»è¦æ˜¯ä¸€äº›apiçš„æœåŠ¡
pip install -r requirements.txt
pip install "httpx[socks]"
```

### 3 Run
```
# depth workflow
cd spagent
python examples/depth_workflow_example_usage.py

```

## ğŸ“Š Evaluation

### prepare BLINK dataset
```
dataset/
â”œâ”€â”€ blink_data.jsonl          # BLINKæ•°æ®é›†æ–‡ä»¶
â””â”€â”€ BLINK/                    # å›¾åƒæ–‡ä»¶å¤¹
    â”œâ”€â”€ 02bf928316cf55ddda3d9e938b89f7624db742364c4dd89eb4e3fddb55f51f9a.jpg
    â”œâ”€â”€ ebb9c1c41b0fe3ff0d65cfc4ef3e2d26e4aefba3be654213a2aeab56d6546443.jpg
    â””â”€â”€ ...
```


### Evaluate gpt-4o-mini on BLINK
```
python spagent/examples/straight_evaluation_gpt.py
```




## ğŸ” External Experts
| å·¥å…·åç§° | ç±»å‹ | ä¸»è¦åŠŸèƒ½ | å¤‡æ³¨ |
| --- | --- | --- | --- |
| **Depth-AnythingV2** | 3D | å•ç›®æ·±åº¦ä¼°è®¡ | å°† 2D å›¾åƒè½¬ä¸ºåƒç´ çº§æ·±åº¦å›¾ |
| **SAM2** | 2D | å›¾åƒåˆ†å‰² | Segment Anything æ¨¡å‹ç¬¬äºŒä»£ï¼Œäº¤äº’å¼æˆ–è‡ªåŠ¨åˆ†å‰² |
| **Supervision** | 2D | è§†è§‰ä»»åŠ¡è¾…åŠ©å·¥å…·åº“ | ç”¨äºç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ç»“æœå¯è§†åŒ–å’Œåå¤„ç† |
| **GroundingDINO** | 2D | æ–‡æœ¬é©±åŠ¨ç›®æ ‡æ£€æµ‹ | åŸºäºè‡ªç„¶è¯­è¨€è¿›è¡Œæ£€æµ‹å’Œæ¡†é€‰ |
| **Pi3** | 3D | ç‚¹äº‘ç”Ÿæˆä¸å¤„ç† | å°†å›¾åƒæˆ–å¤šè§†è§’è¾“å…¥è½¬ä¸º 3D è¡¨ç¤º |


## ğŸ§  Models

| models |
| --- |
| **GPT** |
| **QwenVL** |
| **Local vllm** |

## âœ… Todo





