#!/usr/bin/env python3
"""
ä»æ•°æ®æ–‡ä»¶ä¸­æŒ‰ä»»åŠ¡æŠ½å–æ ·æœ¬æ•°æ®
æ¯ä¸ªä»»åŠ¡æŠ½å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬ï¼Œç”Ÿæˆæ–°çš„sampleæ–‡ä»¶
"""

import json
import random
from collections import defaultdict
import os
import argparse


def extract_samples_by_task(input_file, output_file, samples_per_task=30):
    """
    ä»JSONLæ–‡ä»¶ä¸­æŒ‰ä»»åŠ¡æŠ½å–æ ·æœ¬
    
    Args:
        input_file (str): è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„
        output_file (str): è¾“å‡ºçš„JSONLæ–‡ä»¶è·¯å¾„
        samples_per_task (int): æ¯ä¸ªä»»åŠ¡æŠ½å–çš„æ ·æœ¬æ•°é‡
    """
    
    print(f"ğŸš€ å¼€å§‹ä» {input_file} æŠ½å–æ ·æœ¬æ•°æ®")
    print(f"ğŸ“Š æ¯ä¸ªä»»åŠ¡æŠ½å–: {samples_per_task} ä¸ªæ ·æœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return False
    
    # è¯»å–æ‰€æœ‰æ•°æ®å¹¶æŒ‰ä»»åŠ¡åˆ†ç±»
    task_data = defaultdict(list)
    total_count = 0
    
    print("ğŸ“– è¯»å–æ•°æ®å¹¶æŒ‰ä»»åŠ¡åˆ†ç±»...")
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())
                task = data.get('task', 'unknown')
                task_data[task].append(data)
                total_count += 1
                
                if line_num % 500 == 0:
                    print(f"  å·²è¯»å– {line_num} æ¡æ•°æ®...")
                    
            except json.JSONDecodeError as e:
                print(f"âŒ ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                continue
    
    print(f"âœ… æ•°æ®è¯»å–å®Œæˆï¼Œæ€»è®¡ {total_count} æ¡æ•°æ®")
    
    # æ˜¾ç¤ºä»»åŠ¡åˆ†å¸ƒ
    print(f"\nğŸ“ˆ ä»»åŠ¡åˆ†å¸ƒç»Ÿè®¡:")
    for task, data_list in sorted(task_data.items()):
        print(f"  {task}: {len(data_list)} æ¡")
    
    # ä»æ¯ä¸ªä»»åŠ¡ä¸­æŠ½å–æ ·æœ¬
    print(f"\nğŸ¯ å¼€å§‹æŠ½å–æ ·æœ¬ (æ¯ä¸ªä»»åŠ¡ {samples_per_task} ä¸ª):")
    selected_samples = []
    task_sample_counts = {}
    
    for task, data_list in sorted(task_data.items()):
        available_count = len(data_list)
        sample_count = min(samples_per_task, available_count)
        
        # éšæœºæŠ½å–æ ·æœ¬
        if sample_count > 0:
            sampled_data = random.sample(data_list, sample_count)
            selected_samples.extend(sampled_data)
            task_sample_counts[task] = sample_count
            
            status = "âœ…" if sample_count == samples_per_task else "âš ï¸"
            print(f"  {status} {task}: æŠ½å– {sample_count}/{available_count} ä¸ªæ ·æœ¬")
        else:
            task_sample_counts[task] = 0
            print(f"  âŒ {task}: æ— å¯ç”¨æ•°æ®")
    
    # æŒ‰åŸå§‹é¡ºåºæ’åºï¼ˆåŸºäºIDä¸­çš„ç´¢å¼•ï¼‰
    print(f"\nğŸ“ å¯¹æŠ½å–çš„æ ·æœ¬è¿›è¡Œæ’åº...")
    def extract_idx_from_id(data):
        try:
            return int(data['id'].split('_')[-1])
        except:
            return 0
    
    selected_samples.sort(key=extract_idx_from_id)
    
    # ä¿å­˜æŠ½å–çš„æ ·æœ¬
    print(f"\nğŸ’¾ ä¿å­˜æ ·æœ¬åˆ°: {output_file}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in selected_samples:
            json.dump(sample, f, ensure_ascii=False, separators=(',', ':'))
            f.write('\n')
    
    print(f"âœ… æ ·æœ¬ä¿å­˜å®Œæˆ!")
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ“Š æŠ½å–ç»“æœç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(selected_samples)}")
    print(f"  ä»»åŠ¡æ•°é‡: {len(task_sample_counts)}")
    
    print(f"\nğŸ“ˆ å„ä»»åŠ¡æ ·æœ¬ç»Ÿè®¡:")
    for task, count in sorted(task_sample_counts.items()):
        percentage = (count / samples_per_task * 100) if samples_per_task > 0 else 0
        print(f"  {task}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
    
    # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
    print(f"\nğŸ” éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶...")
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            actual_count = sum(1 for _ in f)
        print(f"  âœ… æ–‡ä»¶éªŒè¯æˆåŠŸï¼Œå®é™…åŒ…å« {actual_count} æ¡æ•°æ®")
    else:
        print(f"  âŒ æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
        return False
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description="ä»æ•°æ®æ–‡ä»¶ä¸­æŒ‰ä»»åŠ¡æŠ½å–æ ·æœ¬æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python create_json_sample.py --input_file dataset/ERQA_All_Data.jsonl --sample 30
  python create_json_sample.py --input_file dataset/cvbench_data.jsonl --sample 50
        """
    )
    
    # æ·»åŠ å¿…éœ€çš„å‚æ•°
    parser.add_argument("--input_file", type=str, required=True, help="è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample", type=int, required=True, help="æ¯ä¸ªä»»åŠ¡æŠ½å–çš„æ ·æœ¬æ•°é‡")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    input_path = args.input_file
    base_name = os.path.splitext(input_path)[0]  # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    extension = os.path.splitext(input_path)[1]  # è·å–æ–‡ä»¶æ‰©å±•å
    output_file = f"{base_name}_sample{args.sample}{extension}"
    
    print("ğŸš€ æ ·æœ¬æŠ½å–å·¥å…·")
    print("=" * 80)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ“Š æ¯ä»»åŠ¡æ ·æœ¬æ•°: {args.sample}")
    print("=" * 80)
    
    # æ‰§è¡ŒæŠ½å–
    success = extract_samples_by_task(
        input_file=args.input_file,
        output_file=output_file,
        samples_per_task=args.sample
    )
    
    print("\n" + "=" * 80)
    if success:
        print("ğŸ‰ æ ·æœ¬æŠ½å–å®Œæˆï¼")
        print(f"âœ… è¾“å…¥æ–‡ä»¶: {args.input_file}")
        print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"âœ… æ¯ä»»åŠ¡æ ·æœ¬æ•°: {args.sample}")
    else:
        print("âŒ æ ·æœ¬æŠ½å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return success


if __name__ == "__main__":
    main()